{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving n-grams from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [N-Gram Based Text Categorization: Categorizing Text With Python by Alejandro Nolla](http://blog.alejandronolla.com/2013/05/20/n-gram-based-text-categorization-categorizing-text-with-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are n-grams? See [here](https://cloudmark.github.io/Language-Detection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves.\"\n",
    "s = s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['le',\n",
       " 'temps',\n",
       " 'est',\n",
       " 'un',\n",
       " 'grand',\n",
       " 'maître',\n",
       " 'dit',\n",
       " 'on',\n",
       " 'le',\n",
       " 'malheur',\n",
       " 'est',\n",
       " \"qu'il\",\n",
       " 'tue',\n",
       " 'ses',\n",
       " 'élèves']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Z'`éèî]+\")\n",
    "s_tokenized = tokenizer.tokenize(s)\n",
    "s_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('_', '_', '_', 'l'),\n",
       "  ('_', '_', 'l', 'e'),\n",
       "  ('_', 'l', 'e', '_'),\n",
       "  ('l', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 't'),\n",
       "  ('_', '_', 't', 'e'),\n",
       "  ('_', 't', 'e', 'm'),\n",
       "  ('t', 'e', 'm', 'p'),\n",
       "  ('e', 'm', 'p', 's'),\n",
       "  ('m', 'p', 's', '_'),\n",
       "  ('p', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')],\n",
       " [('_', '_', '_', 'e'),\n",
       "  ('_', '_', 'e', 's'),\n",
       "  ('_', 'e', 's', 't'),\n",
       "  ('e', 's', 't', '_'),\n",
       "  ('s', 't', '_', '_'),\n",
       "  ('t', '_', '_', '_')],\n",
       " [('_', '_', '_', 'u'),\n",
       "  ('_', '_', 'u', 'n'),\n",
       "  ('_', 'u', 'n', '_'),\n",
       "  ('u', 'n', '_', '_'),\n",
       "  ('n', '_', '_', '_')],\n",
       " [('_', '_', '_', 'g'),\n",
       "  ('_', '_', 'g', 'r'),\n",
       "  ('_', 'g', 'r', 'a'),\n",
       "  ('g', 'r', 'a', 'n'),\n",
       "  ('r', 'a', 'n', 'd'),\n",
       "  ('a', 'n', 'd', '_'),\n",
       "  ('n', 'd', '_', '_'),\n",
       "  ('d', '_', '_', '_')],\n",
       " [('_', '_', '_', 'm'),\n",
       "  ('_', '_', 'm', 'a'),\n",
       "  ('_', 'm', 'a', 'î'),\n",
       "  ('m', 'a', 'î', 't'),\n",
       "  ('a', 'î', 't', 'r'),\n",
       "  ('î', 't', 'r', 'e'),\n",
       "  ('t', 'r', 'e', '_'),\n",
       "  ('r', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 'd'),\n",
       "  ('_', '_', 'd', 'i'),\n",
       "  ('_', 'd', 'i', 't'),\n",
       "  ('d', 'i', 't', '_'),\n",
       "  ('i', 't', '_', '_'),\n",
       "  ('t', '_', '_', '_')],\n",
       " [('_', '_', '_', 'o'),\n",
       "  ('_', '_', 'o', 'n'),\n",
       "  ('_', 'o', 'n', '_'),\n",
       "  ('o', 'n', '_', '_'),\n",
       "  ('n', '_', '_', '_')],\n",
       " [('_', '_', '_', 'l'),\n",
       "  ('_', '_', 'l', 'e'),\n",
       "  ('_', 'l', 'e', '_'),\n",
       "  ('l', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 'm'),\n",
       "  ('_', '_', 'm', 'a'),\n",
       "  ('_', 'm', 'a', 'l'),\n",
       "  ('m', 'a', 'l', 'h'),\n",
       "  ('a', 'l', 'h', 'e'),\n",
       "  ('l', 'h', 'e', 'u'),\n",
       "  ('h', 'e', 'u', 'r'),\n",
       "  ('e', 'u', 'r', '_'),\n",
       "  ('u', 'r', '_', '_'),\n",
       "  ('r', '_', '_', '_')],\n",
       " [('_', '_', '_', 'e'),\n",
       "  ('_', '_', 'e', 's'),\n",
       "  ('_', 'e', 's', 't'),\n",
       "  ('e', 's', 't', '_'),\n",
       "  ('s', 't', '_', '_'),\n",
       "  ('t', '_', '_', '_')],\n",
       " [('_', '_', '_', 'q'),\n",
       "  ('_', '_', 'q', 'u'),\n",
       "  ('_', 'q', 'u', \"'\"),\n",
       "  ('q', 'u', \"'\", 'i'),\n",
       "  ('u', \"'\", 'i', 'l'),\n",
       "  (\"'\", 'i', 'l', '_'),\n",
       "  ('i', 'l', '_', '_'),\n",
       "  ('l', '_', '_', '_')],\n",
       " [('_', '_', '_', 't'),\n",
       "  ('_', '_', 't', 'u'),\n",
       "  ('_', 't', 'u', 'e'),\n",
       "  ('t', 'u', 'e', '_'),\n",
       "  ('u', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 's'),\n",
       "  ('_', '_', 's', 'e'),\n",
       "  ('_', 's', 'e', 's'),\n",
       "  ('s', 'e', 's', '_'),\n",
       "  ('e', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')],\n",
       " [('_', '_', '_', 'é'),\n",
       "  ('_', '_', 'é', 'l'),\n",
       "  ('_', 'é', 'l', 'è'),\n",
       "  ('é', 'l', 'è', 'v'),\n",
       "  ('l', 'è', 'v', 'e'),\n",
       "  ('è', 'v', 'e', 's'),\n",
       "  ('v', 'e', 's', '_'),\n",
       "  ('e', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "generated_4grams = []\n",
    "\n",
    "for word in s_tokenized:\n",
    "    generated_4grams.append(list(ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol=\"_\", right_pad_symbol=\"_\")))\n",
    "generated_4grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `generated_4grams` needs flattening since it's supposed to be a list of 4-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', '_', '_', 'l'),\n",
       " ('_', '_', 'l', 'e'),\n",
       " ('_', 'l', 'e', '_'),\n",
       " ('l', 'e', '_', '_'),\n",
       " ('e', '_', '_', '_'),\n",
       " ('_', '_', '_', 't'),\n",
       " ('_', '_', 't', 'e'),\n",
       " ('_', 't', 'e', 'm'),\n",
       " ('t', 'e', 'm', 'p'),\n",
       " ('e', 'm', 'p', 's')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_4grams = [word for sublist in generated_4grams for word in sublist]\n",
    "generated_4grams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtaining n-grams (n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['___l',\n",
       " '__le',\n",
       " '_le_',\n",
       " 'le__',\n",
       " 'e___',\n",
       " '___t',\n",
       " '__te',\n",
       " '_tem',\n",
       " 'temp',\n",
       " 'emps']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_list_4grams = generated_4grams\n",
    "for idx, val in enumerate(generated_4grams):\n",
    "    ng_list_4grams[idx] = ''.join(val)\n",
    "ng_list_4grams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sorting n-grams by frequency (n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e___', 4),\n",
       " ('s___', 3),\n",
       " ('t___', 3),\n",
       " ('___l', 2),\n",
       " ('__le', 2),\n",
       " ('_le_', 2),\n",
       " ('le__', 2),\n",
       " ('___t', 2),\n",
       " ('___e', 2),\n",
       " ('__es', 2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_4grams = {}\n",
    "\n",
    "for ngram in ng_list_4grams:\n",
    "    if ngram not in freq_4grams:\n",
    "        freq_4grams.update({ngram: 1})\n",
    "    else:\n",
    "        ngram_ocurrences = freq_4grams[ngram]\n",
    "        freq_4grams.update({ngram: ngram_ocurrences + 1})\n",
    "\n",
    "from operator import itemgetter # The operator module exports a set of efficient functions corresponding to the intrinsic\n",
    "# operators of Python. For example, operator.add(x, y) is equivalent to the expression x + y.\n",
    "\n",
    "freq_4grams_sorted = sorted(freq_4grams.items(), key=itemgetter(1), reverse=True)[0:300]\n",
    "# We only keep the 300 most popular n-grams. This was suggested in the original paper written about n-grams.\n",
    "freq_4grams_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Obtaining n-grams for multiple values of n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get n-grams for n = 1, 2, 3 and 4 we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"le temps est un grand maître dit on le malheur est qu'il tue ses élèves\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import everygrams\n",
    "\n",
    "s_clean = ' '.join(s_tokenized) # For the code below we need the raw sentence as opposed to the tokens.\n",
    "s_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l', 'le', 'le_', 'e', 'e_', '_t', '_te', '_tem', 't', 'te']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngram_extractor(sent):\n",
    "    return [''.join(ng) for ng in everygrams(sent.replace(\" \", \"_ _\"), 1, 4)\n",
    "            if \" \" not in ng and \"\\n\" not in ng and ng != (\"_\", )]\n",
    "\n",
    "ngram_extractor(s_clean)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0796340e74cadc37bc856538a7c34bc0799285a1462cf0a6c8f58e59a465e22b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

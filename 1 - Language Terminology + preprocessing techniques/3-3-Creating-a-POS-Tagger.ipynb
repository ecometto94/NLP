{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a classifier to work out which suffixes are most informative for POS tagging. We can begin by finding out what the most common suffixes are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'e': 202946, ',': 175002, '.': 152999, 's': 128722, 'd': 105687, 't': 94459, 'he': 92084, 'n': 87889, 'a': 74912, 'of': 72978, ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk import FreqDist\n",
    "\n",
    "suffix_fdist = FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1\n",
    "    \n",
    "suffix_fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
    "common_suffixes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a feature extractor function which checks a given word for these suffixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'endswith(e)': False,\n",
       " 'endswith(,)': False,\n",
       " 'endswith(.)': False,\n",
       " 'endswith(s)': False,\n",
       " 'endswith(d)': False,\n",
       " 'endswith(t)': True,\n",
       " 'endswith(he)': False,\n",
       " 'endswith(n)': False,\n",
       " 'endswith(a)': False,\n",
       " 'endswith(of)': False,\n",
       " 'endswith(the)': False,\n",
       " 'endswith(y)': False,\n",
       " 'endswith(r)': False,\n",
       " 'endswith(to)': False,\n",
       " 'endswith(in)': False,\n",
       " 'endswith(f)': False,\n",
       " 'endswith(o)': False,\n",
       " 'endswith(ed)': False,\n",
       " 'endswith(nd)': False,\n",
       " 'endswith(is)': False,\n",
       " 'endswith(on)': False,\n",
       " 'endswith(l)': False,\n",
       " 'endswith(g)': False,\n",
       " 'endswith(and)': False,\n",
       " 'endswith(ng)': False,\n",
       " 'endswith(er)': False,\n",
       " 'endswith(as)': False,\n",
       " 'endswith(ing)': False,\n",
       " 'endswith(h)': False,\n",
       " 'endswith(at)': False,\n",
       " 'endswith(es)': False,\n",
       " 'endswith(or)': False,\n",
       " 'endswith(re)': False,\n",
       " 'endswith(it)': False,\n",
       " 'endswith(``)': False,\n",
       " 'endswith(an)': False,\n",
       " \"endswith('')\": False,\n",
       " 'endswith(m)': False,\n",
       " 'endswith(;)': False,\n",
       " 'endswith(i)': False,\n",
       " 'endswith(ly)': False,\n",
       " 'endswith(ion)': False,\n",
       " 'endswith(en)': False,\n",
       " 'endswith(al)': False,\n",
       " 'endswith(?)': False,\n",
       " 'endswith(nt)': False,\n",
       " 'endswith(be)': False,\n",
       " 'endswith(hat)': False,\n",
       " 'endswith(st)': True,\n",
       " 'endswith(his)': False,\n",
       " 'endswith(th)': False,\n",
       " 'endswith(ll)': False,\n",
       " 'endswith(le)': False,\n",
       " 'endswith(ce)': False,\n",
       " 'endswith(by)': False,\n",
       " 'endswith(ts)': False,\n",
       " 'endswith(me)': False,\n",
       " 'endswith(ve)': False,\n",
       " \"endswith(')\": False,\n",
       " 'endswith(se)': False,\n",
       " 'endswith(ut)': False,\n",
       " 'endswith(was)': False,\n",
       " 'endswith(for)': False,\n",
       " 'endswith(ent)': False,\n",
       " 'endswith(ch)': False,\n",
       " 'endswith(k)': False,\n",
       " 'endswith(w)': False,\n",
       " 'endswith(ld)': False,\n",
       " 'endswith(`)': False,\n",
       " 'endswith(rs)': False,\n",
       " 'endswith(ted)': False,\n",
       " 'endswith(ere)': False,\n",
       " 'endswith(her)': False,\n",
       " 'endswith(ne)': False,\n",
       " 'endswith(ns)': False,\n",
       " 'endswith(ith)': False,\n",
       " 'endswith(ad)': False,\n",
       " 'endswith(ry)': False,\n",
       " 'endswith())': False,\n",
       " 'endswith(()': False,\n",
       " 'endswith(te)': False,\n",
       " 'endswith(--)': False,\n",
       " 'endswith(ay)': False,\n",
       " 'endswith(ty)': False,\n",
       " 'endswith(ot)': False,\n",
       " 'endswith(p)': False,\n",
       " 'endswith(nce)': False,\n",
       " \"endswith('s)\": False,\n",
       " 'endswith(ter)': False,\n",
       " 'endswith(om)': False,\n",
       " 'endswith(ss)': False,\n",
       " 'endswith(:)': False,\n",
       " 'endswith(we)': False,\n",
       " 'endswith(are)': False,\n",
       " 'endswith(c)': False,\n",
       " 'endswith(ers)': False,\n",
       " 'endswith(uld)': False,\n",
       " 'endswith(had)': False,\n",
       " 'endswith(so)': False,\n",
       " 'endswith(ey)': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "    return features\n",
    "\n",
    "pos_features('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our feature extractor, we can use it to train a new decision tree classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'endswith(e)': True,\n",
       "  'endswith(,)': False,\n",
       "  'endswith(.)': False,\n",
       "  'endswith(s)': False,\n",
       "  'endswith(d)': False,\n",
       "  'endswith(t)': False,\n",
       "  'endswith(he)': True,\n",
       "  'endswith(n)': False,\n",
       "  'endswith(a)': False,\n",
       "  'endswith(of)': False,\n",
       "  'endswith(the)': True,\n",
       "  'endswith(y)': False,\n",
       "  'endswith(r)': False,\n",
       "  'endswith(to)': False,\n",
       "  'endswith(in)': False,\n",
       "  'endswith(f)': False,\n",
       "  'endswith(o)': False,\n",
       "  'endswith(ed)': False,\n",
       "  'endswith(nd)': False,\n",
       "  'endswith(is)': False,\n",
       "  'endswith(on)': False,\n",
       "  'endswith(l)': False,\n",
       "  'endswith(g)': False,\n",
       "  'endswith(and)': False,\n",
       "  'endswith(ng)': False,\n",
       "  'endswith(er)': False,\n",
       "  'endswith(as)': False,\n",
       "  'endswith(ing)': False,\n",
       "  'endswith(h)': False,\n",
       "  'endswith(at)': False,\n",
       "  'endswith(es)': False,\n",
       "  'endswith(or)': False,\n",
       "  'endswith(re)': False,\n",
       "  'endswith(it)': False,\n",
       "  'endswith(``)': False,\n",
       "  'endswith(an)': False,\n",
       "  \"endswith('')\": False,\n",
       "  'endswith(m)': False,\n",
       "  'endswith(;)': False,\n",
       "  'endswith(i)': False,\n",
       "  'endswith(ly)': False,\n",
       "  'endswith(ion)': False,\n",
       "  'endswith(en)': False,\n",
       "  'endswith(al)': False,\n",
       "  'endswith(?)': False,\n",
       "  'endswith(nt)': False,\n",
       "  'endswith(be)': False,\n",
       "  'endswith(hat)': False,\n",
       "  'endswith(st)': False,\n",
       "  'endswith(his)': False,\n",
       "  'endswith(th)': False,\n",
       "  'endswith(ll)': False,\n",
       "  'endswith(le)': False,\n",
       "  'endswith(ce)': False,\n",
       "  'endswith(by)': False,\n",
       "  'endswith(ts)': False,\n",
       "  'endswith(me)': False,\n",
       "  'endswith(ve)': False,\n",
       "  \"endswith(')\": False,\n",
       "  'endswith(se)': False,\n",
       "  'endswith(ut)': False,\n",
       "  'endswith(was)': False,\n",
       "  'endswith(for)': False,\n",
       "  'endswith(ent)': False,\n",
       "  'endswith(ch)': False,\n",
       "  'endswith(k)': False,\n",
       "  'endswith(w)': False,\n",
       "  'endswith(ld)': False,\n",
       "  'endswith(`)': False,\n",
       "  'endswith(rs)': False,\n",
       "  'endswith(ted)': False,\n",
       "  'endswith(ere)': False,\n",
       "  'endswith(her)': False,\n",
       "  'endswith(ne)': False,\n",
       "  'endswith(ns)': False,\n",
       "  'endswith(ith)': False,\n",
       "  'endswith(ad)': False,\n",
       "  'endswith(ry)': False,\n",
       "  'endswith())': False,\n",
       "  'endswith(()': False,\n",
       "  'endswith(te)': False,\n",
       "  'endswith(--)': False,\n",
       "  'endswith(ay)': False,\n",
       "  'endswith(ty)': False,\n",
       "  'endswith(ot)': False,\n",
       "  'endswith(p)': False,\n",
       "  'endswith(nce)': False,\n",
       "  \"endswith('s)\": False,\n",
       "  'endswith(ter)': False,\n",
       "  'endswith(om)': False,\n",
       "  'endswith(ss)': False,\n",
       "  'endswith(:)': False,\n",
       "  'endswith(we)': False,\n",
       "  'endswith(are)': False,\n",
       "  'endswith(c)': False,\n",
       "  'endswith(ers)': False,\n",
       "  'endswith(uld)': False,\n",
       "  'endswith(had)': False,\n",
       "  'endswith(so)': False,\n",
       "  'endswith(ey)': False},\n",
       " 'AT')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words = brown.tagged_words(categories='news')\n",
    "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n",
    "featuresets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import DecisionTreeClassifier\n",
    "from nltk.classify import accuracy\n",
    "\n",
    "cutoff = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[cutoff:], featuresets[:cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'endswith(e)': False,\n",
       "  'endswith(,)': False,\n",
       "  'endswith(.)': False,\n",
       "  'endswith(s)': False,\n",
       "  'endswith(d)': False,\n",
       "  'endswith(t)': False,\n",
       "  'endswith(he)': False,\n",
       "  'endswith(n)': False,\n",
       "  'endswith(a)': False,\n",
       "  'endswith(of)': False,\n",
       "  'endswith(the)': False,\n",
       "  'endswith(y)': False,\n",
       "  'endswith(r)': True,\n",
       "  'endswith(to)': False,\n",
       "  'endswith(in)': False,\n",
       "  'endswith(f)': False,\n",
       "  'endswith(o)': False,\n",
       "  'endswith(ed)': False,\n",
       "  'endswith(nd)': False,\n",
       "  'endswith(is)': False,\n",
       "  'endswith(on)': False,\n",
       "  'endswith(l)': False,\n",
       "  'endswith(g)': False,\n",
       "  'endswith(and)': False,\n",
       "  'endswith(ng)': False,\n",
       "  'endswith(er)': False,\n",
       "  'endswith(as)': False,\n",
       "  'endswith(ing)': False,\n",
       "  'endswith(h)': False,\n",
       "  'endswith(at)': False,\n",
       "  'endswith(es)': False,\n",
       "  'endswith(or)': False,\n",
       "  'endswith(re)': False,\n",
       "  'endswith(it)': False,\n",
       "  'endswith(``)': False,\n",
       "  'endswith(an)': False,\n",
       "  \"endswith('')\": False,\n",
       "  'endswith(m)': False,\n",
       "  'endswith(;)': False,\n",
       "  'endswith(i)': False,\n",
       "  'endswith(ly)': False,\n",
       "  'endswith(ion)': False,\n",
       "  'endswith(en)': False,\n",
       "  'endswith(al)': False,\n",
       "  'endswith(?)': False,\n",
       "  'endswith(nt)': False,\n",
       "  'endswith(be)': False,\n",
       "  'endswith(hat)': False,\n",
       "  'endswith(st)': False,\n",
       "  'endswith(his)': False,\n",
       "  'endswith(th)': False,\n",
       "  'endswith(ll)': False,\n",
       "  'endswith(le)': False,\n",
       "  'endswith(ce)': False,\n",
       "  'endswith(by)': False,\n",
       "  'endswith(ts)': False,\n",
       "  'endswith(me)': False,\n",
       "  'endswith(ve)': False,\n",
       "  \"endswith(')\": False,\n",
       "  'endswith(se)': False,\n",
       "  'endswith(ut)': False,\n",
       "  'endswith(was)': False,\n",
       "  'endswith(for)': False,\n",
       "  'endswith(ent)': False,\n",
       "  'endswith(ch)': False,\n",
       "  'endswith(k)': False,\n",
       "  'endswith(w)': False,\n",
       "  'endswith(ld)': False,\n",
       "  'endswith(`)': False,\n",
       "  'endswith(rs)': False,\n",
       "  'endswith(ted)': False,\n",
       "  'endswith(ere)': False,\n",
       "  'endswith(her)': False,\n",
       "  'endswith(ne)': False,\n",
       "  'endswith(ns)': False,\n",
       "  'endswith(ith)': False,\n",
       "  'endswith(ad)': False,\n",
       "  'endswith(ry)': False,\n",
       "  'endswith())': False,\n",
       "  'endswith(()': False,\n",
       "  'endswith(te)': False,\n",
       "  'endswith(--)': False,\n",
       "  'endswith(ay)': False,\n",
       "  'endswith(ty)': False,\n",
       "  'endswith(ot)': False,\n",
       "  'endswith(p)': False,\n",
       "  'endswith(nce)': False,\n",
       "  \"endswith('s)\": False,\n",
       "  'endswith(ter)': False,\n",
       "  'endswith(om)': False,\n",
       "  'endswith(ss)': False,\n",
       "  'endswith(:)': False,\n",
       "  'endswith(we)': False,\n",
       "  'endswith(are)': False,\n",
       "  'endswith(c)': False,\n",
       "  'endswith(ers)': False,\n",
       "  'endswith(uld)': False,\n",
       "  'endswith(had)': False,\n",
       "  'endswith(so)': False,\n",
       "  'endswith(ey)': False},\n",
       " 'PP$')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# NLTK is a teaching toolkit which is not really optimized for speed. \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Therefore, this may take forever. For speed, use scikit-learn for the classifiers.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m accuracy(classifier, test_set)\n",
      "File \u001b[1;32mc:\\Users\\ecometto001\\Documents\\Personal\\NLP\\.venv\\lib\\site-packages\\nltk\\classify\\decisiontree.py:175\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.train\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    170\u001b[0m     tree \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier\u001b[38;5;241m.\u001b[39mbest_binary_stump(\n\u001b[0;32m    171\u001b[0m         feature_names, labeled_featuresets, feature_values, verbose\n\u001b[0;32m    172\u001b[0m     )\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Refine the stump.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabeled_featuresets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_cutoff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43msupport_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Return it\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32mc:\\Users\\ecometto001\\Documents\\Personal\\NLP\\.venv\\lib\\site-packages\\nltk\\classify\\decisiontree.py:231\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.refine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    229\u001b[0m     label_freqs \u001b[38;5;241m=\u001b[39m FreqDist(label \u001b[38;5;28;01mfor\u001b[39;00m (featureset, label) \u001b[38;5;129;01min\u001b[39;00m fval_featuresets)\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m entropy(MLEProbDist(label_freqs)) \u001b[38;5;241m>\u001b[39m entropy_cutoff:\n\u001b[1;32m--> 231\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decisions[fval] \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfval_featuresets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m            \u001b[49m\u001b[43mentropy_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdepth_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43msupport_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     default_featuresets \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    242\u001b[0m         (featureset, label)\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (featureset, label) \u001b[38;5;129;01min\u001b[39;00m labeled_featuresets\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m featureset\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fname) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decisions\n\u001b[0;32m    245\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ecometto001\\Documents\\Personal\\NLP\\.venv\\lib\\site-packages\\nltk\\classify\\decisiontree.py:175\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.train\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    170\u001b[0m     tree \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier\u001b[38;5;241m.\u001b[39mbest_binary_stump(\n\u001b[0;32m    171\u001b[0m         feature_names, labeled_featuresets, feature_values, verbose\n\u001b[0;32m    172\u001b[0m     )\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Refine the stump.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabeled_featuresets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_cutoff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43msupport_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Return it\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32mc:\\Users\\ecometto001\\Documents\\Personal\\NLP\\.venv\\lib\\site-packages\\nltk\\classify\\decisiontree.py:231\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.refine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    229\u001b[0m     label_freqs \u001b[38;5;241m=\u001b[39m FreqDist(label \u001b[38;5;28;01mfor\u001b[39;00m (featureset, label) \u001b[38;5;129;01min\u001b[39;00m fval_featuresets)\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m entropy(MLEProbDist(label_freqs)) \u001b[38;5;241m>\u001b[39m entropy_cutoff:\n\u001b[1;32m--> 231\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decisions[fval] \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfval_featuresets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m            \u001b[49m\u001b[43mentropy_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdepth_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43msupport_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     default_featuresets \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    242\u001b[0m         (featureset, label)\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (featureset, label) \u001b[38;5;129;01min\u001b[39;00m labeled_featuresets\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m featureset\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fname) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decisions\n\u001b[0;32m    245\u001b[0m     ]\n",
      "    \u001b[1;31m[... skipping similar frames: DecisionTreeClassifier.refine at line 231 (1 times), DecisionTreeClassifier.train at line 175 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ecometto001\\Documents\\Personal\\NLP\\.venv\\lib\\site-packages\\nltk\\classify\\decisiontree.py:175\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.train\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    170\u001b[0m     tree \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier\u001b[38;5;241m.\u001b[39mbest_binary_stump(\n\u001b[0;32m    171\u001b[0m         feature_names, labeled_featuresets, feature_values, verbose\n\u001b[0;32m    172\u001b[0m     )\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Refine the stump.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabeled_featuresets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_cutoff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43msupport_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Return it\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32mc:\\Users\\ecometto001\\Documents\\Personal\\NLP\\.venv\\lib\\site-packages\\nltk\\classify\\decisiontree.py:231\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.refine\u001b[1;34m(self, labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    229\u001b[0m     label_freqs \u001b[38;5;241m=\u001b[39m FreqDist(label \u001b[38;5;28;01mfor\u001b[39;00m (featureset, label) \u001b[38;5;129;01min\u001b[39;00m fval_featuresets)\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m entropy(MLEProbDist(label_freqs)) \u001b[38;5;241m>\u001b[39m entropy_cutoff:\n\u001b[1;32m--> 231\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decisions[fval] \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfval_featuresets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m            \u001b[49m\u001b[43mentropy_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdepth_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43msupport_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     default_featuresets \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    242\u001b[0m         (featureset, label)\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (featureset, label) \u001b[38;5;129;01min\u001b[39;00m labeled_featuresets\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m featureset\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fname) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decisions\n\u001b[0;32m    245\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ecometto001\\Documents\\Personal\\NLP\\.venv\\lib\\site-packages\\nltk\\classify\\decisiontree.py:166\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.train\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Start with a stump.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m binary:\n\u001b[1;32m--> 166\u001b[0m     tree \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_stump\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabeled_featuresets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     tree \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier\u001b[38;5;241m.\u001b[39mbest_binary_stump(\n\u001b[0;32m    171\u001b[0m         feature_names, labeled_featuresets, feature_values, verbose\n\u001b[0;32m    172\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ecometto001\\Documents\\Personal\\NLP\\.venv\\lib\\site-packages\\nltk\\classify\\decisiontree.py:263\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.best_stump\u001b[1;34m(feature_names, labeled_featuresets, verbose)\u001b[0m\n\u001b[0;32m    261\u001b[0m best_error \u001b[38;5;241m=\u001b[39m best_stump\u001b[38;5;241m.\u001b[39merror(labeled_featuresets)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m feature_names:\n\u001b[1;32m--> 263\u001b[0m     stump \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabeled_featuresets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m     stump_error \u001b[38;5;241m=\u001b[39m stump\u001b[38;5;241m.\u001b[39merror(labeled_featuresets)\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stump_error \u001b[38;5;241m<\u001b[39m best_error:\n",
      "File \u001b[1;32mc:\\Users\\ecometto001\\Documents\\Personal\\NLP\\.venv\\lib\\site-packages\\nltk\\classify\\decisiontree.py:201\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.stump\u001b[1;34m(feature_name, labeled_featuresets)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m featureset, label \u001b[38;5;129;01min\u001b[39;00m labeled_featuresets:\n\u001b[0;32m    200\u001b[0m     feature_value \u001b[38;5;241m=\u001b[39m featureset\u001b[38;5;241m.\u001b[39mget(feature_name)\n\u001b[1;32m--> 201\u001b[0m     freqs[feature_value][label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    203\u001b[0m decisions \u001b[38;5;241m=\u001b[39m {val: DecisionTreeClassifier(freqs[val]\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m freqs}\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DecisionTreeClassifier(label, feature_name, decisions)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# classifier = DecisionTreeClassifier.train(train_set) # NLTK is a teaching toolkit which is not really optimized for speed. \n",
    "# Therefore, this may take forever. For speed, use scikit-learn for the classifiers.\n",
    "# accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.classify(pos_features('cats'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0796340e74cadc37bc856538a7c34bc0799285a1462cf0a6c8f58e59a465e22b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
